{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Appendix A\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "290693ae04fb631b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regex\n",
    "\n",
    "[Regular expressions](https://docs.python.org/3/library/re.html), also known as regex or regexp, are a powerful tool for manipulating text. They are widely used in various programming languages, including Python. Regular expressions match patterns in strings, allowing for advanced data extraction, data manipulation, and data validation.\n",
    "\n",
    "### Pros of Using Regular Expressions:\n",
    "\n",
    "**Pattern Matching:** Regular expression allows you to select specific patterns in a text rather than matching exact text. For instance, you can filter out valid email addresses or telephone numbers from a document.\n",
    "\n",
    "**Compactness:** Regular expressions can handle a complex text manipulating task in a single line of code.\n",
    "\n",
    "**Flexibility:** With regular expressions, you can handle many different text scenarios due to a wide range of special characters and operators.\n",
    "\n",
    "**Powerful Tools:** Many programming languages, text editors, and database systems support regular expressions. They are indispensable for advanced text processing tasks.\n",
    "\n",
    "Here's an example of a regular expression in Python that matches an email address:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de2b260f9f8f987a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid email!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\"\n",
    "email = \"example@example.com\"\n",
    "match = re.match(pattern, email)\n",
    "if match:\n",
    "    print(\"Valid email!\")\n",
    "else:\n",
    "    print(\"Invalid email!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T20:41:15.448940159Z",
     "start_time": "2024-01-10T20:41:15.445894840Z"
    }
   },
   "id": "1c382c0ee236be87",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cons of Using Regular Expressions:\n",
    "\n",
    "**Complexity:** Regular expressions can become extremely complex and hard to read, especially for those who are new to programming or unfamiliar with regex syntax.\n",
    "\n",
    "**Difficulty in Debugging:** If a regular expression has a small mistake, it could lead to major problems, and these problems can be hard to debug due to the cryptic nature of regular expression syntax.\n",
    "\n",
    "**Performance:** Regular expressions can be slower than other methods of string manipulation, especially for more complex expressions or large pieces of text."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "594de91a659000bc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pandas Regex\n",
    "\n",
    "Pandas is a popular data manipulation library in Python and it comes with vectorized string functions which are capable of performing operations on entire series of data. These vectorized string functions are a part of pandas and have the power of regular expressions and are also equipped to handle missing values (NaN).\n",
    "\n",
    "To get started with regular expressions in pandas, you need to import the pandas library first:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c67a4ec16f74aded"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T20:41:16.447359468Z",
     "start_time": "2024-01-10T20:41:15.449350121Z"
    }
   },
   "id": "cd479825236fd757",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "A common use case of regular expressions in pandas is when you want to filter data in your dataframe. For example, let's suppose we have a DataFrame with some email data:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d60c3f0a5a51166"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "    name            email\n0   John     john@abc.com\n1   Anna     anna@xyz.com\n2  Peter        peter@123\n3  Linda  linda@lmnop.com",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>email</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>John</td>\n      <td>john@abc.com</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Anna</td>\n      <td>anna@xyz.com</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Peter</td>\n      <td>peter@123</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Linda</td>\n      <td>linda@lmnop.com</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'name': ['John', 'Anna', 'Peter', 'Linda'],\n",
    "        'email': ['john@abc.com', 'anna@xyz.com', 'peter@123', 'linda@lmnop.com']}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T20:41:16.448703432Z",
     "start_time": "2024-01-10T20:41:16.446761988Z"
    }
   },
   "id": "9faf841c56ecb1cc",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you want to filter out rows where email column contains valid email addresses, you can use the .str.contains method with a regular expression pattern:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85740b03c679f380"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "    name            email\n0   John     john@abc.com\n1   Anna     anna@xyz.com\n3  Linda  linda@lmnop.com",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>email</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>John</td>\n      <td>john@abc.com</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Anna</td>\n      <td>anna@xyz.com</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Linda</td>\n      <td>linda@lmnop.com</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\"\n",
    "valid_emails_df = df[df['email'].str.contains(pattern)]\n",
    "valid_emails_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T20:41:16.449274902Z",
     "start_time": "2024-01-10T20:41:16.446998068Z"
    }
   },
   "id": "6a82bf233315e38c",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another useful method is .str.extract, which extracts a part of the string that matches the regular expression pattern. For example, if you want to extract the domain from the email addresses, you can do:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c6ca1f2ec1a5385"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "    name            email     domain\n0   John     john@abc.com    abc.com\n1   Anna     anna@xyz.com    xyz.com\n2  Peter        peter@123        123\n3  Linda  linda@lmnop.com  lmnop.com",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>email</th>\n      <th>domain</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>John</td>\n      <td>john@abc.com</td>\n      <td>abc.com</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Anna</td>\n      <td>anna@xyz.com</td>\n      <td>xyz.com</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Peter</td>\n      <td>peter@123</td>\n      <td>123</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Linda</td>\n      <td>linda@lmnop.com</td>\n      <td>lmnop.com</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['domain'] = df['email'].str.extract(r'@([a-zA-Z0-9-.]+)')\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T20:41:16.450088734Z",
     "start_time": "2024-01-10T20:41:16.447152182Z"
    }
   },
   "id": "93a40f0dd06f5162",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    ".str.replace method can be used to replace the parts matching regular expression with some other string. For instance, if you desire to replace the domain in all emails to \"newdomain.com\", here is how you can do it:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ba227ac1b204211"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "    name                email     domain\n0   John   john@newdomain.com    abc.com\n1   Anna   anna@newdomain.com    xyz.com\n2  Peter  peter@newdomain.com        123\n3  Linda  linda@newdomain.com  lmnop.com",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>email</th>\n      <th>domain</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>John</td>\n      <td>john@newdomain.com</td>\n      <td>abc.com</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Anna</td>\n      <td>anna@newdomain.com</td>\n      <td>xyz.com</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Peter</td>\n      <td>peter@newdomain.com</td>\n      <td>123</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Linda</td>\n      <td>linda@newdomain.com</td>\n      <td>lmnop.com</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['email'] = df['email'].str.replace(r'@[a-zA-Z0-9-.]+', '@newdomain.com', regex=True)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T20:41:16.460964332Z",
     "start_time": "2024-01-10T20:41:16.447291208Z"
    }
   },
   "id": "2ad7ed7afa38e790",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Above, we made use of a column replace function. Note that the same function can be applied to an entire dataframe as well.\n",
    "\n",
    "Keep in mind, while these vectorized functions greatly simplify data preprocessing, they can be a little difficult to debug due to complexities of regular expression syntax. Therefore, it's important to thoroughly test your regular expressions to ensure they're correctly matching the patterns you need.\n",
    "\n",
    "Also note that while the regex language is somewhat similar between implementations, its not exact.  Perl regex and python regex are both slightly different in what they can do, and how to specify them.  Mixing syntax can be a pain to debug, so be careful."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51ff49733dc7e5f2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Function Definitions\n",
    "\n",
    "In Python, a function is a block of reusable code that is used to perform a specific related group of actions. Functions help us break our program into smaller and modular blocks for better organization and code reusability.\n",
    "\n",
    "To define a function, Python provides the def keyword. Here's the syntax for defining a function in Python:\n",
    "\n",
    "```python\n",
    "def function_name(parameters):\n",
    "    \"\"\"docstring\"\"\"\n",
    "    statement(s)\n",
    "```\n",
    "\n",
    "* **function_name:** This is the name of the function, it should be unique and descriptive of what the function does.\n",
    "* **parameters:** These are optional values that the function uses to perform its operations. If a function has parameters, you can pass different arguments each time you call it.\n",
    "* **docstring:** It is optional and used to describe what the function does.\n",
    "* **statement(s):** This is the code that the function executes when it's called.\n",
    "\n",
    "For example:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c4981d91a2c74f2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def greet(name):\n",
    "    \"\"\"This function greets the person passed in as argument\"\"\"\n",
    "    print(\"Hello, \" + name + \". Good morning!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T20:41:16.519260086Z",
     "start_time": "2024-01-10T20:41:16.461132142Z"
    }
   },
   "id": "44916cb258f3ea3b",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "Global variables in Python are those variables that are declared outside of a function. They can be accessed by any function in the program, making them useful for storing values that multiple functions might need to access.\n",
    "\n",
    "Here's a simple example of a global variable:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68677de09efcbb8e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "x = 10  # This is a global variable\n",
    "\n",
    "def print_global():\n",
    "    print(x)  # We can access the global variable x inside this function\n",
    "\n",
    "print_global()  # Prints: 10"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T20:41:16.624200084Z",
     "start_time": "2024-01-10T20:41:16.476549775Z"
    }
   },
   "id": "80ed5a30081f99c",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "While global variables can be very useful, they should be used sparingly because they can make it harder to understand how your program works. If you have a lot of functions that change the global state, it can be hard to keep track of all the changes.\n",
    "\n",
    "If you really need to use a global variable inside of a function, you can do so with the global keyword:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48c9e69e9bc65d64"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "x = 10\n",
    "\n",
    "def modify_global():\n",
    "    global x  # This tells Python we're going to work with the global x\n",
    "    x = 20  # This changes the global x, not a local one\n",
    "\n",
    "modify_global()\n",
    "print(x)  # Prints: 20, because we changed the global x inside the function"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T20:41:16.625053322Z",
     "start_time": "2024-01-10T20:41:16.517467446Z"
    }
   },
   "id": "44e1dc51161663c",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remember that using global variables can make your code harder to understand and maintain, so use them sparingly and only when necessary. It's usually better to pass values into a function as parameters and get results back as a return value."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d82808348a718f29"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Functions must be declared before they are used.  So, keeping with our linear flow idea of how to organize a notebook, in general, you will want to define your functions in cells that occur before they are used.\n",
    "\n",
    "In practice, I tend to just write code in cells until I find that I need to use the code more than once, and then I define a function, extract and generalize the code into the new function, only then deleting the original cells.  The cells are a perfect place to experiment with the code and its behavior.\n",
    "\n",
    "As mentioned above, I try to avoid using global variables, and make sure to pass all the needed data into my functions.  Often, when migrating from a cell to a function, it is common to end up with local variables named the same as what were global variables.  As a result, as you turn that into a function, it is very easy to miss one of those variables and not turn them into a newly named function parameter.  The code will still work, as the global variable you were referencing still exists.  I dont have any tricks to avoiding this, except to suggest being careful, these kinds of errors are hard to detect.\n",
    "\n",
    "A very contrived example: "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d74b05371f3f0f17"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[2, 8]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list = [8,2,3,4,5]\n",
    "lngth = 2\n",
    "\n",
    "# original cell contents\n",
    "my_list2 = sorted(my_list[:lngth])\n",
    "my_list2\n",
    "           "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T20:41:16.626162097Z",
     "start_time": "2024-01-10T20:41:16.517696623Z"
    }
   },
   "id": "bb3a7f022f97fcb",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "When I try to turn that into a function, I might do the following:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28f5942839786079"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[2, 8]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list = [8,2,3,4,5]\n",
    "lngth = 2\n",
    "\n",
    "def get_first_N_sorted(thelist):\n",
    "    my_list2 = sorted(thelist[:lngth])\n",
    "    return my_list2\n",
    "\n",
    "get_first_N_sorted(my_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T20:41:16.671575575Z",
     "start_time": "2024-01-10T20:41:16.521859294Z"
    }
   },
   "id": "1b4823bba085c15b",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note how I forgot to pass in the `lngth` parameter.  My function above will be using the global lngth, and should I change it, my function will begin to return different elements than before.  This can take a while to find in the real world."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff25d68ed5ee0bf0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[2, 3, 8]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lngth=3\n",
    "get_first_N_sorted(my_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T20:41:16.672314795Z",
     "start_time": "2024-01-10T20:41:16.563501096Z"
    }
   },
   "id": "75e9fa417ec14c4c",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "What we should have written was:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93606f37820301cb"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[2, 8]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_first_N_sorted(thelist, thelen):\n",
    "    return sorted(thelist[:thelen])\n",
    "\n",
    "my_list = [8,2,3,4,5]\n",
    "\n",
    "get_first_N_sorted(my_list, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T20:41:16.672844465Z",
     "start_time": "2024-01-10T20:41:16.563771071Z"
    }
   },
   "id": "c9a17190cdeae0b",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Type Conversion\n",
    "\n",
    "Pandas provides several data structures like Series, DataFrame, and Panel. These data structures work with a variety of data types. Pandas automatically assigns data types when reading in data or converting data. However, sometimes you may need to explicitly convert data types. This process is known as Type Conversion.\n",
    "\n",
    "Pandas support seven types of data, including:\n",
    "\n",
    "* **object:** Used for string or mixed data types.\n",
    "* **int64:** 64-bit integer data type.\n",
    "* **float64:** Floating point number.\n",
    "* **bool:** Used for Boolean values True/False.\n",
    "* **datetime64:** Used for date and time, without timezone.\n",
    "* **timedelta:** Used for representing differences in dates and times (seconds, minutes, etc.).\n",
    "* **category:** Used for categorical variables.\n",
    "\n",
    "Data Type Conversion\n",
    "You can change type of a specific column using astype() function. For example:\n",
    "\n",
    "```python\n",
    "df['column_name'] = df['column_name'].astype('data_type')\n",
    "```\n",
    "\n",
    "Where 'column_name' is column you intended to convert and 'data_type' is your desired data type.\n",
    "Sometimes, conversion to a certain type may fail if the column contains inappropriate data (e.g., trying to convert a column of strings to integers when some strings don't represent integers). In this case, a ValueError is raised.\n",
    "\n",
    "```python\n",
    "# Convert object type to integer\n",
    "# Pretend the Age column contains the word 'One', 'Two', 'Three', etc.\n",
    "df['Age'] = df['Age'].astype('int') \n",
    "```\n",
    "\n",
    "Note that in addition to the per column `astype` method above, there is an `astype` method that applies to an entire dataframe.  The parameter to the function is a dictionary, where you specify as the key the column name and the value is the type you want to convert to.  This allows you to change many columns at once.\n",
    "\n",
    "### convert_dtypes Method\n",
    "\n",
    "Pandas has a method to convert columns to the best possible dtypes using the new dtypes introduced in pandas.\n",
    "\n",
    "Here is how it can be used:\n",
    "\n",
    "```python\n",
    "df = df.convert_dtypes()\n",
    "```\n",
    "\n",
    "Internally it tries to do these conversions:\n",
    "\n",
    "* convert integer columns from int64 to the smallest possible integer dtype (pd.Int64Dtype() or similar, depending on the size)\n",
    "* convert float columns to the Nullable Float type (pd.Float64Dtype())\n",
    "* convert object dtype to the appropriate dtype if the column has a dictionary-like data or is a string.\n",
    "\n",
    "### Nullable Types\n",
    "\n",
    "One important feature to note is the difference between traditional ('non-nullable') types and the new 'nullable' types introduced in pandas. These new types can have an optional Boolean mask that indicates if a value is missing, which allows for both missing and non-missing data to be accommodated in the same data type.\n",
    "\n",
    "For example, a traditional integer array cannot have null values. Pandas uses a special representation for such missing values (often np.nan). However, data manipulation operations like groupbys, joins, and reshaping can introduce missing values. With the introduction of Nullable Integer Data Types, integers can now have true missing values. This makes the nullable types (Int64, float64, boolean) more flexible.\n",
    "\n",
    "Each of pandas' data types has a corresponding nullable version, for example pd.Int32 instead of int32, which should be used after converting data types using the convert_dtypes method. These hold missing values using a separate mask, and when the result of an operation on the data cannot be represented in the data type, a missing value is produced even when the data type supports the operation.\n",
    "\n",
    "Note: The reason pandas represent nullable types with a capital letter (for example, 'Int64'), is to differentiate them from the python types which don't support null values. The python 'int' type, for example, is represented as 'int64' in pandas.\n",
    "\n",
    "### Why Type Conversion\n",
    "\n",
    "When working with pandas DataFrames, data types play an important role not only in data manipulation and analysis, but also in optimizing performance and memory usage. Converting the data types in your DataFrame to their most appropriate or 'least-cost' types can bring a number of significant benefits.\n",
    "\n",
    "#### Efficiency in Memory Usage:\n",
    "\n",
    "Different data types in pandas use different amounts of memory. For example, an int64 type uses 64 bits of memory, while an int8 uses only 8 bits. If you know that the integer data you're working with fits within the range of an int8 (-128 to 127), using int8 instead of int64 can lead to substantial memory savings. This can be particularly beneficial when working with large datasets, where memory limitations can cause a bottleneck.\n",
    "\n",
    "The same principle applies to other data types. For example, converting objects (which are often used to store strings) to categorical data (when the number of distinct values is limited) can significantly reduce the memory footprint.\n",
    "\n",
    "#### Performance Improvement:\n",
    "\n",
    "Utilizing less memory also tends to speed up computations and operations on the data. Certain operations are also optimized for certain data types. For example, operations on numerical data types are usually faster than those on object (string) types. Thus, converting columns to the correct data type can result in better performance.\n",
    "\n",
    "#### Data Consistency and Accuracy:\n",
    "\n",
    "Using the appropriate data type can also ensure data consistency and reduce the risk of errors. For instance, using a boolean data type for a column containing True/False values prevents this column from accidentally being treated as numeric (where True is interpreted as 1 and False as 0).\n",
    "\n",
    "#### Enabling DataFrame functionalities:\n",
    "\n",
    "Certain pandas functionalities only work with specific data types. For example, the .mean() method will not work on a column of strings - the column must be numeric data type (such as int or float). By ensuring the correct data type for each column, you'll be able to make full use of pandas functionalities.\n",
    "\n",
    "Although data type conversion might seem like an extra step, it is an important aspect of pre-processing your data. It helps in efficient memory utilization, speed improvement, ensuring data accuracy and unlocking additional DataFrame functionalities. Hence, it is a good practice to always check and convert data to appropriate types when working with pandas DataFrames.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25a42da8f6eaaf56"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Styling Tables\n",
    "\n",
    "Using Pandas and Jupyter Notebook, you can apply [conditional formatting and styling](https://pandas.pydata.org/pandas-docs/stable/user_guide/style.html) to your dataframes that can be very useful for data analysis and visual representation. One of these features is to color cells based on their values or display a bar chart in cell background.\n",
    "\n",
    "**Note that this really only works in the web jupyter interface, and any other tool set that just displays the raw html that the dataframe creates.  IntelliJ, for instance, takes over all table rendering and will ignore any styles set**\n",
    "\n",
    "Here is how you can do it:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7bcbb44b62f76b6"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "    A   B   C\n0  75  65  90\n1  85  85  80\n2  90  80  80\n3  80  75  85\n4  70  95  90\n5  80  85  80",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>A</th>\n      <th>B</th>\n      <th>C</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>75</td>\n      <td>65</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>85</td>\n      <td>85</td>\n      <td>80</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>90</td>\n      <td>80</td>\n      <td>80</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>80</td>\n      <td>75</td>\n      <td>85</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>70</td>\n      <td>95</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>80</td>\n      <td>85</td>\n      <td>80</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'A': [75, 85, 90, 80, 70, 80],\n",
    "    'B': [65, 85, 80, 75, 95, 85],\n",
    "    'C': [90, 80, 80, 85, 90, 80]\n",
    "})\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T20:41:16.674543156Z",
     "start_time": "2024-01-10T20:41:16.573763454Z"
    }
   },
   "id": "7aa42ae9ce4324c3",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "Color Cells Based on Their Values\n",
    "To color the cells based on their value, use the Styler.background_gradient method. This function applies a gradient coloring to the DataFrame values:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "959a5a0e58a6e107"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<pandas.io.formats.style.Styler at 0x7fe840390500>",
      "text/html": "<style type=\"text/css\">\n#T_186ed_row0_col0 {\n  background-color: #c6dbef;\n  color: #000000;\n}\n#T_186ed_row0_col1, #T_186ed_row1_col2, #T_186ed_row2_col2, #T_186ed_row4_col0, #T_186ed_row5_col2 {\n  background-color: #f7fbff;\n  color: #000000;\n}\n#T_186ed_row0_col2, #T_186ed_row2_col0, #T_186ed_row4_col1, #T_186ed_row4_col2 {\n  background-color: #08306b;\n  color: #f1f1f1;\n}\n#T_186ed_row1_col0 {\n  background-color: #2070b4;\n  color: #f1f1f1;\n}\n#T_186ed_row1_col1, #T_186ed_row5_col1 {\n  background-color: #3787c0;\n  color: #f1f1f1;\n}\n#T_186ed_row2_col1, #T_186ed_row3_col0, #T_186ed_row3_col2, #T_186ed_row5_col0 {\n  background-color: #6aaed6;\n  color: #f1f1f1;\n}\n#T_186ed_row3_col1 {\n  background-color: #abd0e6;\n  color: #000000;\n}\n</style>\n<table id=\"T_186ed\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_186ed_level0_col0\" class=\"col_heading level0 col0\" >A</th>\n      <th id=\"T_186ed_level0_col1\" class=\"col_heading level0 col1\" >B</th>\n      <th id=\"T_186ed_level0_col2\" class=\"col_heading level0 col2\" >C</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_186ed_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n      <td id=\"T_186ed_row0_col0\" class=\"data row0 col0\" >75</td>\n      <td id=\"T_186ed_row0_col1\" class=\"data row0 col1\" >65</td>\n      <td id=\"T_186ed_row0_col2\" class=\"data row0 col2\" >90</td>\n    </tr>\n    <tr>\n      <th id=\"T_186ed_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n      <td id=\"T_186ed_row1_col0\" class=\"data row1 col0\" >85</td>\n      <td id=\"T_186ed_row1_col1\" class=\"data row1 col1\" >85</td>\n      <td id=\"T_186ed_row1_col2\" class=\"data row1 col2\" >80</td>\n    </tr>\n    <tr>\n      <th id=\"T_186ed_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n      <td id=\"T_186ed_row2_col0\" class=\"data row2 col0\" >90</td>\n      <td id=\"T_186ed_row2_col1\" class=\"data row2 col1\" >80</td>\n      <td id=\"T_186ed_row2_col2\" class=\"data row2 col2\" >80</td>\n    </tr>\n    <tr>\n      <th id=\"T_186ed_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n      <td id=\"T_186ed_row3_col0\" class=\"data row3 col0\" >80</td>\n      <td id=\"T_186ed_row3_col1\" class=\"data row3 col1\" >75</td>\n      <td id=\"T_186ed_row3_col2\" class=\"data row3 col2\" >85</td>\n    </tr>\n    <tr>\n      <th id=\"T_186ed_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n      <td id=\"T_186ed_row4_col0\" class=\"data row4 col0\" >70</td>\n      <td id=\"T_186ed_row4_col1\" class=\"data row4 col1\" >95</td>\n      <td id=\"T_186ed_row4_col2\" class=\"data row4 col2\" >90</td>\n    </tr>\n    <tr>\n      <th id=\"T_186ed_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n      <td id=\"T_186ed_row5_col0\" class=\"data row5 col0\" >80</td>\n      <td id=\"T_186ed_row5_col1\" class=\"data row5 col1\" >85</td>\n      <td id=\"T_186ed_row5_col2\" class=\"data row5 col2\" >80</td>\n    </tr>\n  </tbody>\n</table>\n"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.style.background_gradient(cmap='Blues')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T20:41:17.392479535Z",
     "start_time": "2024-01-10T20:41:16.614953444Z"
    }
   },
   "id": "58ea1a36ed8334b6",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<pandas.io.formats.style.Styler at 0x7fe839d20530>",
      "text/html": "<style type=\"text/css\">\n#T_c1167_row0_col0 {\n  width: 10em;\n  background: linear-gradient(90deg, transparent 50.0%, blue 50.0%, blue 91.7%, transparent 91.7%);\n}\n#T_c1167_row0_col1 {\n  width: 10em;\n  background: linear-gradient(90deg, transparent 50.0%, blue 50.0%, blue 84.2%, transparent 84.2%);\n}\n#T_c1167_row0_col2, #T_c1167_row2_col0, #T_c1167_row4_col1, #T_c1167_row4_col2 {\n  width: 10em;\n  background: linear-gradient(90deg, transparent 50.0%, blue 50.0%, blue 100.0%, transparent 100.0%);\n}\n#T_c1167_row1_col0, #T_c1167_row3_col2 {\n  width: 10em;\n  background: linear-gradient(90deg, transparent 50.0%, blue 50.0%, blue 97.2%, transparent 97.2%);\n}\n#T_c1167_row1_col1, #T_c1167_row5_col1 {\n  width: 10em;\n  background: linear-gradient(90deg, transparent 50.0%, blue 50.0%, blue 94.7%, transparent 94.7%);\n}\n#T_c1167_row1_col2, #T_c1167_row2_col2, #T_c1167_row3_col0, #T_c1167_row5_col0, #T_c1167_row5_col2 {\n  width: 10em;\n  background: linear-gradient(90deg, transparent 50.0%, blue 50.0%, blue 94.4%, transparent 94.4%);\n}\n#T_c1167_row2_col1 {\n  width: 10em;\n  background: linear-gradient(90deg, transparent 50.0%, blue 50.0%, blue 92.1%, transparent 92.1%);\n}\n#T_c1167_row3_col1 {\n  width: 10em;\n  background: linear-gradient(90deg, transparent 50.0%, blue 50.0%, blue 89.5%, transparent 89.5%);\n}\n#T_c1167_row4_col0 {\n  width: 10em;\n  background: linear-gradient(90deg, transparent 50.0%, blue 50.0%, blue 88.9%, transparent 88.9%);\n}\n</style>\n<table id=\"T_c1167\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_c1167_level0_col0\" class=\"col_heading level0 col0\" >A</th>\n      <th id=\"T_c1167_level0_col1\" class=\"col_heading level0 col1\" >B</th>\n      <th id=\"T_c1167_level0_col2\" class=\"col_heading level0 col2\" >C</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_c1167_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n      <td id=\"T_c1167_row0_col0\" class=\"data row0 col0\" >75</td>\n      <td id=\"T_c1167_row0_col1\" class=\"data row0 col1\" >65</td>\n      <td id=\"T_c1167_row0_col2\" class=\"data row0 col2\" >90</td>\n    </tr>\n    <tr>\n      <th id=\"T_c1167_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n      <td id=\"T_c1167_row1_col0\" class=\"data row1 col0\" >85</td>\n      <td id=\"T_c1167_row1_col1\" class=\"data row1 col1\" >85</td>\n      <td id=\"T_c1167_row1_col2\" class=\"data row1 col2\" >80</td>\n    </tr>\n    <tr>\n      <th id=\"T_c1167_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n      <td id=\"T_c1167_row2_col0\" class=\"data row2 col0\" >90</td>\n      <td id=\"T_c1167_row2_col1\" class=\"data row2 col1\" >80</td>\n      <td id=\"T_c1167_row2_col2\" class=\"data row2 col2\" >80</td>\n    </tr>\n    <tr>\n      <th id=\"T_c1167_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n      <td id=\"T_c1167_row3_col0\" class=\"data row3 col0\" >80</td>\n      <td id=\"T_c1167_row3_col1\" class=\"data row3 col1\" >75</td>\n      <td id=\"T_c1167_row3_col2\" class=\"data row3 col2\" >85</td>\n    </tr>\n    <tr>\n      <th id=\"T_c1167_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n      <td id=\"T_c1167_row4_col0\" class=\"data row4 col0\" >70</td>\n      <td id=\"T_c1167_row4_col1\" class=\"data row4 col1\" >95</td>\n      <td id=\"T_c1167_row4_col2\" class=\"data row4 col2\" >90</td>\n    </tr>\n    <tr>\n      <th id=\"T_c1167_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n      <td id=\"T_c1167_row5_col0\" class=\"data row5 col0\" >80</td>\n      <td id=\"T_c1167_row5_col1\" class=\"data row5 col1\" >85</td>\n      <td id=\"T_c1167_row5_col2\" class=\"data row5 col2\" >80</td>\n    </tr>\n  </tbody>\n</table>\n"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.style.bar(color='blue', align='zero')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T20:41:17.398880276Z",
     "start_time": "2024-01-10T20:41:17.396432967Z"
    }
   },
   "id": "13b6d116724073de",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "The align parameter aligns the bars at the zero value for the columns.\n",
    "\n",
    "Please note that these visualizations don't modify your data, and are just ways to visualize the data for any sort of analysis or debugging.\n",
    "\n",
    "Both of these methods return a Styler object, which has useful methods for formatting and displaying DataFrames. The styles are built up incrementally, which means you can concatenate a bunch of style methods together and create a good looking dataframe!\n",
    "\n",
    "Remember to make sure that your Jupyter Notebook is capable of displaying HTML output for these styles to render."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e5a33e1f259e759"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Advanced usage of parquet files.\n",
    "\n",
    "### A discussion of categorical data in pandas\n",
    "\n",
    "Categorical data is a feature provided by pandas for managing categorical variables, which are variables that can take on one of a limited set of values, called categories. An example of a categorical feature could be the color of a car, which can be red, blue, black, etc.\n",
    "\n",
    "#### Reasons for Using Categorical Data:\n",
    "\n",
    "**Memory Efficiency:** Categorical data type utilizes significantly less memory by assigning numerical identifiers to distinct categories. This can be especially beneficial when dealing with large datasets with numerous repeating categories.\n",
    "\n",
    "**Performance Enhancement:** Because categorical data uses less memory, the computations and operations performed on categorical data are often faster than the same operations performed on, for instance, objects data type.\n",
    "\n",
    "**Maintaining Ordinal Relationship:** The categorical data type in pandas also supports ordered categories. This means you can express an inherent order or hierarchy within your categories, such as 'low', 'medium', 'high' for a response variable.\n",
    "\n",
    "***Beneficial for certain statistical methods:** Certain statistical methods and machine learning algorithms require categorical data to function correctly. Using pandas' Categorical data type ensures compatibility with these methods.\n",
    "\n",
    "Usage Code Example:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "s = pd.Series([\"aaaaa\",\"bbbbb\",\"cccccc\",\"aaaaa\"], dtype=\"category\")\n",
    "```\n",
    "\n",
    "In this case, the series uses less memory and computation on this series will be faster compared to a non-categorical series with \"object\" dtype.\n",
    "\n",
    "#### Memory Benefit:\n",
    "\n",
    "The memory usage of a Categorical is proportional to the number of categories plus the length of the data. In contrast, a regular Series with data type \"object\" has a memory footprint proportional to the length of the data times the length of the strings. So when the categories are a small fraction of the total number of values, memory usage will be significantly less.\n",
    "\n",
    "#### Potential Negatives:\n",
    "\n",
    "**Inflexibility:** Once you've set your categorical data, it's a bit more complex to change the categories compared to manipulating a regular Series. For example, adding a new category requires the \"Categorical\" to be recategorized.\n",
    "\n",
    "**Complexity:** Understanding the categorical data type and using it correctly adds a layer of complexity to your code, making it potentially harder to write and understand.\n",
    "\n",
    "**Compatibility Issues:** Not all pandas operations are compatible with the categorical data type. Some functions might not work, or might convert the categorical data back to a regular series.\n",
    "\n",
    "### Parquet Files\n",
    "\n",
    "Many IO operations do not know what a categorical data series is.  The output will be expressed with fully expanded values.  While the dataframe may store small numbers for a set of IP's in a dataframe, when they are displayed or saved as text, the full IP will be shown/saved.  This is perfect for interchange, but if you want to save small versions of the files, this is not ideal.\n",
    "\n",
    "The parquet data file format does understand how to save categorical data correctly though, so if your trying to save data for personal use and you want to take as little space as you can, parquet should be your go to file format.\n",
    "\n",
    "Parquet can save categorical data correctly, but it helps if you provide the save operation a few hints as to which columns are categorical and which arent.\n",
    "\n",
    "```python\n",
    "df.to_parquet(\n",
    "    filename, # the file to save to\n",
    "    version=2.6, # make sure its a new enough version of parquet to handle everything\n",
    "    compression=\"brotli\", # specify what kind of compression.  See docs for details, there \n",
    "                          # are many but I find brotli makes the smallest files.\n",
    "    use_dictionary=sorted(set(df.select_dtypes(include=['category']).columns))\n",
    "    # The above is designed to tell the parquet code to use a dictionary to save \n",
    "    # any data that exists in a column with type category.  As with a dataframe, with\n",
    "    # with this setting, instead of saving 100,000 ip addresses (of which there are only 4\n",
    "    # distinct ones, the saved data will contain a dictionary of just the 4 unique addresses\n",
    "    # and the dataframe will just contain the much smaller index values.  This will likely also\n",
    "    # compress better, in addition to just naturally taking up less space.. \n",
    ")\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ac0b55d50023281"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Caching\n",
    "\n",
    "Caching is something you find us doing from time to time, purely as a quality of life improvement to our software.\n",
    "\n",
    "In short, im willing to wait 20 minutes to download a lot of data from the server once, I am not willing to do it 100s of times a day while im developing my notebooks.\n",
    "\n",
    "To that end, sometimes it is useful to either:\n",
    "\n",
    "* Create one notebook to download, clean, and save data and then write a second notebook to process the data.\n",
    "* Write your notebook to download, clean and save data, only if it cant find a copy of that data on local disk.  If a stored version exists, use that instead.\n",
    "\n",
    "The idea is, do the hard work to download and clean data just once, save it off, and work with that from there on. It will be much faster than repeatedly fetching data remotely.\n",
    "\n",
    "Caching Pandas dataframes in local storage essentially means saving the dataframe to a file (like CSV, Parquet, etc.) for future use. This operation can have several benefits as well as potential drawbacks:\n",
    "\n",
    "### Benefits of Caching Pandas DataFrames in Local Storage:\n",
    "\n",
    "**Reuse:** Data preprocessing can be time-consuming. If you have a large raw dataset and once you've cleaned it and gotten it into the right shape, saving your processed DataFrame to a file allows you to re-use it without having to repeat the entire preprocessing step.\n",
    "\n",
    "**Sharing DataFrames:** If you need to share your data with colleagues, or move it between different environments (like development, testing, and production), saving it to a file is a very convenient way to do that.\n",
    "\n",
    "**Saving Processed Data:** If you're working with data that's collected in real-time and you're maintaining a running DataFrame of processed data, regularly saving to a file can ensure you don't lose your data if the process is interrupted.\n",
    "\n",
    "**Frequency of Access:** Especially for large dataframes, if you read the same dataframe multiple times in your program, it could be beneficial to store it in your local cache to speed up the subsequent reads.\n",
    "\n",
    "### Drawbacks of Caching Pandas DataFrames in Local Storage:\n",
    "\n",
    "**Storage Space:** Large DataFrames can take up a lot of storage space. If you're working with multiple large dataframes and you save all of them to files, you might fill up your storage space quickly.\n",
    "\n",
    "**I/O Operations:** Reading from and writing to a file involves disk I/O operations which are significantly slower than operations in memory. So, if your DataFrame fits comfortably in memory and you don't need the benefits listed above, it might be faster to not save it to a file.  That being said, its almost always faster to read from truly local storage than from a remote database or api server, so keep that in mind as well.\n",
    "\n",
    "**Data Secrecy:** Storing data in local storage exposes it to the risk of being accessed by unauthorized users, especially when dealing with sensitive data.\n",
    "\n",
    "**Data Consistency:** Frequent read/write operations can lead to data inconsistency issues, especially in multi-threaded/multi-user environments.\n",
    "\n",
    "The choice of whether to cache pandas DataFrames in local storage or not depends on the specifics of your project, such as the size of your data, the cost of preprocessing, the frequency of data access, and storage capacity.\n",
    "\n",
    "Most of our caching is done by writing dataframes to storage as parquet files, as shown above.  \n",
    "\n",
    "There are a few things to think about if you decide to cache data, aside from what is above.\n",
    "\n",
    "It is very easy to mess up with cached data and read in old data when you really wanted new stuff.  If I write a function that accepts parameters, looks for a local file, and uses it if its there, I need to make sure that the file selection takes those parameters into consideration.  An example below.\n",
    "\n",
    "\n",
    "```python\n",
    "def get_data(start_date, end_date, filterstring):\n",
    "    if os.path.exists('cachefile.pq'):\n",
    "        return pd.read_parquet('cachefile.pq')\n",
    "    \n",
    "    df = pd.DataFrame(server.query(start_date, end_date, filterstring))\n",
    "    df.to_parquet('cachefile.pq')\n",
    "    return df\n",
    "    \n",
    "```\n",
    "\n",
    "The above function will use the cache file if it exists, else, it will run the query save it as a parquet file, and return the data.\n",
    "\n",
    "Note however, that the `get_data` function accepts parameters, a start date, end date, and a filter string.  Those will be used if the cache file doesnt exist, but from there on, they will be ignored and the old data will be returned.  This is not what we want.  We want new parameters to cause new queries to be run.  Re-using old parameters should get us the saved data from past runs with the exact parameters, nothing else.\n",
    "\n",
    "The easiest way to accomplish this is to make sure you include the parameters in the file name.\n",
    "\n",
    "```python\n",
    "def get_data(start_date, end_date, filterstring):\n",
    "    cache_file = f\"cachfile_{start_date}_{end_date}_{filterstring}.pq\"\n",
    "    if os.path.exists(cache_file):\n",
    "        return pd.read_parquet(cache_file)\n",
    "    \n",
    "    df = pd.DataFrame(server.query(start_date, end_date, filterstring))\n",
    "    df.to_parquet(cache_file)\n",
    "    return df    \n",
    "```\n",
    "\n",
    "Now, every time we change the parameters, we will check, not find a cache file, and run the query.  We will end up with one cache file for each set of parameters.\n",
    "\n",
    "Note that above, we really should have done something to makesure the cache filename was valid, that the date string didnt corrupt it or the like.  To do this, sometimes I choose not to use real variable contents in the file name, but to concat them all together and calcualte a hash of them.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70e790ada5f9b771"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed076287532e86365e841e92bfc50d8c\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "# Define the strings \n",
    "arguments = ['Hello', ' ', 'World', '!']\n",
    "\n",
    "# Initialize a new md5 hash object\n",
    "hash_object = hashlib.md5()\n",
    "\n",
    "# Iterate over the strings and add each one to the hash object\n",
    "for s in arguments:\n",
    "    hash_object.update(s.encode()) # Ensure the string is in bytes\n",
    "\n",
    "# Extract the hexdigest\n",
    "hex_dig = hash_object.hexdigest()\n",
    "\n",
    "print(hex_dig)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T20:41:17.428735081Z",
     "start_time": "2024-01-10T20:41:17.399117558Z"
    }
   },
   "id": "854f0623ff10725d",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "With code like the above, I can take all the arguments to my query system and create a unique hash of them, and make that part of the cache filename.  This will guarantee that if I change any of the input parameters, I will also change the cache filename, meaning that there is little to no chance of me reading in old data meant for a different invocation of the function.\n",
    "\n",
    "It's important to note that the hashlib.md5() method is often not recommended for cryptographic uses due to vulnerabilities in MD5. Nonetheless, it remains popular for checksums and data integrity verification in non-security-critical applications.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b7fe6915fc5d914"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T20:41:17.457683056Z",
     "start_time": "2024-01-10T20:41:17.413391870Z"
    }
   },
   "id": "fd38780d1abe9285",
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
