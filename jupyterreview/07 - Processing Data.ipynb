{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Processing Data\n",
    "\n",
    "Earlier, we mentioned that most of our notebooks, and even some of our individual cells, follow a fairly standard pattern:\n",
    "\n",
    "1. Read Data\n",
    "2. Clean Data\n",
    "3. Filter Data\n",
    "4. **Process Data**\n",
    "5. Output Data\n",
    "\n",
    "This article will be about processing data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43d45367b6d94cf4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is not uncommon for a python notebook or cell to not actually do any data processing.  Sometimes, all you want to do is read, clean, and filter the data before output.  You may, for instance, want to read in a pcap file, clean it up, filter it down to just conversations involving a particular ip address, and then display the data, or possibly graph the payload size over time, etc.\n",
    "\n",
    "In those cases, there is no explicit data processing stage.\n",
    "\n",
    "When there is a data processing stage, I find that 9 times out of 10, it going to end up being a group by/aggregation operation.  Most transformations we have done to the data is to create summary reports on our data which involve aggregating things up. We may resample the data into larger time series chunks, or we may want to split our data up by network direction and calculate summary statistics.  Most of the transformations we do to the data after filtering is to create higher level summaries of the data.  And that falls generally to the `groupby` function and its relatives in the pandas library"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4300637dedbb9af0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "One of the most powerful and commonly used features in Pandas is the `groupby` operation. This function essentially allows you to split your data into groups, apply a function to each group independently, and then combine the results into an output DataFrame.\n",
    "\n",
    "Let's break it down a bit more:\n",
    "\n",
    "### Split-Apply-Combine\n",
    "\n",
    "The groupby operation follows what is known as the split-apply-combine pattern.\n",
    "\n",
    "#### Split: \n",
    "\n",
    "First, data is split into groups based on some criteria. For instance, if we have a dataset with information about different cities in various countries, we could group our data by 'Country'. This will create a separate group for each unique country in our dataset.\n",
    "\n",
    "#### Apply: \n",
    "\n",
    "After the data is split into groups, you then apply a function to each group. The function could be a standard aggregation function (like mean, min, max, sum, count, etc.), or a custom function defined by you. The function is applied independently to each group.\n",
    "\n",
    "#### Combine: \n",
    "\n",
    "Finally, the resulting groups are combined back into a new DataFrame, with the group identifier as the index and the applied function results as the DataFrame values.\n",
    "\n",
    "### Usage\n",
    "\n",
    "Here's an example of how to use the groupby function in Pandas:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b9cb8fd12467509"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country\n",
      "Canada     5130000\n",
      "USA       15300000\n",
      "Name: Population, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create a simple dataframe\n",
    "data = {\n",
    "    'Country': ['USA', 'USA', 'Canada', 'Canada', 'USA', 'Canada'],\n",
    "    'City': ['New York', 'Los Angeles', 'Toronto', 'Montreal', 'Chicago', 'Vancouver'],\n",
    "    'Population': [8600000, 4000000, 2800000, 1700000, 2700000, 630000],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# group by country and find the sum of populations in each country\n",
    "grouped = df.groupby('Country')['Population'].sum()\n",
    "\n",
    "print(grouped)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T00:05:02.333999600Z",
     "start_time": "2023-12-21T00:05:01.070087400Z"
    }
   },
   "id": "9797233343a29e8a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Potential Issues\n",
    "\n",
    "While groupby is an incredibly useful function, there are some potential pitfalls to keep in mind:\n",
    "\n",
    "#### Missing values: \n",
    "\n",
    "If there are missing values in the columns you want to group by, they will be excluded from the groups. You may want to handle missing values before applying groupby.\n",
    "\n",
    "#### Aggregation over non-numeric data: \n",
    "\n",
    "If you attempt to apply a numeric function (like mean, sum, etc.) over non-numeric data, you'll get an error. Make sure your data types are appropriate for the aggregation function you want to apply.\n",
    "\n",
    "#### Large datasets: \n",
    "\n",
    "The groupby operation can become resource-intensive with very large datasets. If you notice a slowdown when using this function, consider optimizing your use of the groupby operation or employ other optimizations like working with a subset of your data or using different data types to save on memory.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27642671e530a41c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "apply() is another powerful Pandas method used alongside groupby() to perform complex operations on grouped data that can't be achieved with standard aggregation functions.\n",
    "\n",
    "The apply() function applies a function along any axis of a DataFrame. When we use apply() with groupby(), we can apply any particular function to each group of values.\n",
    "\n",
    "Here's how to use the apply() function:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f07c6ca0dbda38b4"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country\n",
      "Canada    5637.362637\n",
      "USA       5728.191689\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create a simple data frame\n",
    "data = {\n",
    "    'Country': ['USA', 'USA', 'Canada', 'Canada', 'USA', 'Canada'],\n",
    "    'City': ['New York', 'Los Angeles', 'Toronto', 'Montreal', 'Chicago', 'Vancouver'],\n",
    "    'Population': [8600000, 4000000, 2800000, 1700000, 2700000, 630000],\n",
    "    'Area': [780, 1302, 630, 165, 589, 115]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define a function that calculates population density\n",
    "def calculate_density(xdata):\n",
    "    # Population density is population divided by area\n",
    "    return xdata['Population'].sum() / xdata['Area'].sum()\n",
    "\n",
    "grouped = df.groupby('Country').apply(calculate_density)\n",
    "print(grouped)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T00:05:02.353366500Z",
     "start_time": "2023-12-21T00:05:02.327005Z"
    }
   },
   "id": "17b19a4e332ba89b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this example, calculate_density function is applied to each group in the DataFrame separately, and the results are combined into a new DataFrame.\n",
    "\n",
    "**Points to note:**\n",
    "\n",
    "The function passed to apply() should expect a DataFrame (representing the data for a group) as its argument and return a Series, a DataFrame or a scalar.\n",
    "\n",
    "apply() can be used to perform more complex operations than aggregation functions and transform(). However, it should be noted that apply() can be slower than the specialized functions (like transform() or agg()) and should be used when the operation cannot be performed by the others.\n",
    "\n",
    "New users to Pandas often get confused by how to use apply() and groupby() together, so it's worth spending some time practicing with different examples to fully understand how they work together. Depending on the use case, the combination can provide a high level of flexibility."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d23ddcbced6c354e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Apply\n",
    "\n",
    "The other operations that I find in the data processing section of a notebook/cell is the application of functions over rows, and merging/joining dataframes.\n",
    "\n",
    "Lets look at applying a function over every row in a dataframe."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c984a75f97ca3b3"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "  Country         City  Population  Area\n0     USA     New York     8600000   780\n1     USA  Los Angeles     4000000  1302\n2  Canada      Toronto     2800000   630\n3  Canada     Montreal     1700000   165\n4     USA      Chicago     2700000   589\n5  Canada    Vancouver      630000   115",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Country</th>\n      <th>City</th>\n      <th>Population</th>\n      <th>Area</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>USA</td>\n      <td>New York</td>\n      <td>8600000</td>\n      <td>780</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>USA</td>\n      <td>Los Angeles</td>\n      <td>4000000</td>\n      <td>1302</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Canada</td>\n      <td>Toronto</td>\n      <td>2800000</td>\n      <td>630</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Canada</td>\n      <td>Montreal</td>\n      <td>1700000</td>\n      <td>165</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>USA</td>\n      <td>Chicago</td>\n      <td>2700000</td>\n      <td>589</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Canada</td>\n      <td>Vancouver</td>\n      <td>630000</td>\n      <td>115</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T00:05:02.524035900Z",
     "start_time": "2023-12-21T00:05:02.340673700Z"
    }
   },
   "id": "e4238d3d89eaf7f0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this experiment, I am going to create a fifth column that contains the md5 hash of all the values in each row.  \n",
    "\n",
    "Python has a hashlib that will do this, but it expects single strings/bytes, not an entire column of values.  So, we will have to apply the function to each row."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea1853ab24517458"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "'6a433feb54ba001ebc1f385ef9e9de84'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "hashlib.md5(\"ASDFASDFASD\".encode()).hexdigest()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T00:07:15.382921100Z",
     "start_time": "2023-12-21T00:07:15.361569300Z"
    }
   },
   "id": "66de6a8dbb1f6ccb"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "  Country         City  Population  Area                              hash\n0     USA     New York     8600000   780  acc43c3a25694f9b72961483cf0ea5e5\n1     USA  Los Angeles     4000000  1302  626b1e66a5ebf04321a66c459f49b73a\n2  Canada      Toronto     2800000   630  68e8eba6b9f92295f191952fb514a90d\n3  Canada     Montreal     1700000   165  e6710b15df4f8e2def5147bf8cd28c24\n4     USA      Chicago     2700000   589  d9fe39ed1d90342cdedb185e34ea4d7f\n5  Canada    Vancouver      630000   115  5b770e8a2152cfdb45e3aa05187a197f",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Country</th>\n      <th>City</th>\n      <th>Population</th>\n      <th>Area</th>\n      <th>hash</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>USA</td>\n      <td>New York</td>\n      <td>8600000</td>\n      <td>780</td>\n      <td>acc43c3a25694f9b72961483cf0ea5e5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>USA</td>\n      <td>Los Angeles</td>\n      <td>4000000</td>\n      <td>1302</td>\n      <td>626b1e66a5ebf04321a66c459f49b73a</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Canada</td>\n      <td>Toronto</td>\n      <td>2800000</td>\n      <td>630</td>\n      <td>68e8eba6b9f92295f191952fb514a90d</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Canada</td>\n      <td>Montreal</td>\n      <td>1700000</td>\n      <td>165</td>\n      <td>e6710b15df4f8e2def5147bf8cd28c24</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>USA</td>\n      <td>Chicago</td>\n      <td>2700000</td>\n      <td>589</td>\n      <td>d9fe39ed1d90342cdedb185e34ea4d7f</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Canada</td>\n      <td>Vancouver</td>\n      <td>630000</td>\n      <td>115</td>\n      <td>5b770e8a2152cfdb45e3aa05187a197f</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.assign(hash = lambda df: df.apply(lambda r: hashlib.md5((r.Country + r.City + str(r.Population) + str(r.Area)).encode()).hexdigest(), axis='columns'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T00:09:11.111917200Z",
     "start_time": "2023-12-21T00:09:11.088628700Z"
    }
   },
   "id": "dc736f153b876f10"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The assign function, which we have seen before, accepts a function that takes a dataframe.  For many operations thats all you need, but in this case, we need to apply a function across every row.  This is done with the:\n",
    "\n",
    "```python\n",
    "df.assign(hash = lambda xf: xf.apply(fun, axis='columns'))\n",
    "```\n",
    "\n",
    "The assign function accepts a simple function that will return a series based on the xf dataframe passed in.  To generate this series, we use the xf.apply method, with a function we want to run and the axis over which we want to run it. This is a bit confusing, but you say axis='columns' when you want the function to process all the columns of a single row. You can pass axis='rows' if you want to apply the function over every column of a row.  I have never used axis='rows', only columns, and I expect that is true for most people. \n",
    "\n",
    "The function we pass to apply ends up being a function that receives a row, and can use all of the various columns on that row.\n",
    "\n",
    "```python\n",
    "lambda r: hashlib.md5(r.Country, r.City, ...)\n",
    "```\n",
    "\n",
    "is applied to each row of the dataframe, and the resulting values are put togeter in to a series that gets attached to hash via the assign statement.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "603014109d19e73a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Merge/Join\n",
    "\n",
    "Merge and join are two important operations that are often done in data manipulation and analysis.\n",
    "\n",
    "###### Pandas Merge\n",
    "\n",
    "Pandas merge connects columns or indexes in DataFrame based on one or more keys. It provides a very flexible interface, allowing you to merge on indexes or columns and offers various types of set operations to use during the merge.\n",
    "\n",
    "Here's the basic syntax:\n",
    "\n",
    "```python\n",
    "pd.merge(left, right, how='inner', on=None, left_on=None, right_on=None,\n",
    "         left_index=False, right_index=False, sort=True)\n",
    "```\n",
    "* left: A DataFrame object.\n",
    "* right: Another DataFrame object.\n",
    "* on: Columns (names) to join on. Must be found in both the left and right DataFrame objects.\n",
    "* left_on: Columns from the left DataFrame to use as keys.\n",
    "* right_on: Columns from the right DataFrame to use as keys.\n",
    "* left_index: If True, use the index (row labels) from the left DataFrame as its join key(s).\n",
    "* right_index: Same usage as left_index for the right DataFrame.\n",
    "* how: One of 'inner', 'outer', 'left', or 'right'. Default is 'inner'. Each method has certain attributes of set theory.\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "```python\n",
    "merged_df = pd.merge(df1, df2, on='id')\n",
    "```\n",
    "\n",
    "This will merge two dataframes, df1 and df2, based on the id column.\n",
    "\n",
    "#### Join\n",
    "\n",
    "Pandas DataFrame join is a convenient method for combining the columns of two DataFrames. This is quite similar to the merge method but the join method will be performed on the columns on the default index.\n",
    "\n",
    "Here is a basic syntax of join:\n",
    "\n",
    "```python\n",
    "df.join(other, on=None, how='left', lsuffix='', rsuffix='', sort=False)\n",
    "```\n",
    "\n",
    "* other: DataFrame, Series with name field set, or dict-like\n",
    "* on: Column or index level names to join on in the DataFrame.\n",
    "* how: How to handle the operation of the two objects. 1. 'left': Use keys from left frame only 2. 'right': Use keys from right frame only 3. 'outer': Use union of keys from both frames 4. 'inner': Use intersection of keys from both frames\n",
    "* lsuffix: Suffix to use from left frame’s overlapping columns\n",
    "* rsuffix: Suffix to use from right frame’s overlapping columns\n",
    "\n",
    "Some general rules to remember:\n",
    "\n",
    "You would use merge() when you want to merge on a particular column and you can specify this column.\n",
    "\n",
    "You would use join() when you are joining on index or you want to join on the basis of the index of the right frame.\n",
    "\n",
    "Keep in mind that in most cases, merge and join can be used interchangeably, it largely depends on the specific use-case and the layout of your data.\n",
    "\n",
    "Lets do a more concrete example."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4da050630139e4ae"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets take our city and country data and join it with a list of branch locations to work out the country and population of each branches area."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfe604f51b23a744"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "  Name         City\n0   B1     New York\n1   B2  Los Angeles\n2   B3      Chicago\n3   B4    Vancouver",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>City</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>B1</td>\n      <td>New York</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>B2</td>\n      <td>Los Angeles</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>B3</td>\n      <td>Chicago</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>B4</td>\n      <td>Vancouver</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "branches = pd.DataFrame({'Name': ['B1', 'B2', 'B3', 'B4'], 'City': ['New York', 'Los Angeles', 'Chicago', 'Vancouver']})\n",
    "branches"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T00:22:04.115479300Z",
     "start_time": "2023-12-21T00:22:04.088382800Z"
    }
   },
   "id": "97bcf2ea83548fab"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "  Name         City Country  Population  Area\n0   B1     New York     USA     8600000   780\n1   B2  Los Angeles     USA     4000000  1302\n2   B3      Chicago     USA     2700000   589\n3   B4    Vancouver  Canada      630000   115",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>City</th>\n      <th>Country</th>\n      <th>Population</th>\n      <th>Area</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>B1</td>\n      <td>New York</td>\n      <td>USA</td>\n      <td>8600000</td>\n      <td>780</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>B2</td>\n      <td>Los Angeles</td>\n      <td>USA</td>\n      <td>4000000</td>\n      <td>1302</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>B3</td>\n      <td>Chicago</td>\n      <td>USA</td>\n      <td>2700000</td>\n      <td>589</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>B4</td>\n      <td>Vancouver</td>\n      <td>Canada</td>\n      <td>630000</td>\n      <td>115</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "branches.merge(df, left_on='City', right_on='City', how='inner')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T00:22:29.990471200Z",
     "start_time": "2023-12-21T00:22:29.959773900Z"
    }
   },
   "id": "2cdc9f77f6614604"
  },
  {
   "cell_type": "markdown",
   "source": [
    "And now we have all the branches coupled with their city, country, and pop all in one dataframe.\n",
    "\n",
    "Joining and merging are fundamental operations in data manipulations and cleaning. Here are reasons why one would want to use join or merge pandas methods:\n",
    "\n",
    "**Combining data:** You might have data spread across multiple sources and you need to bring it together. This is the most common use case.\n",
    "\n",
    "**Missing Data:** You might want to join or merge dataframes based on specific columns to fill in the missing values in one DataFrame using the values from another DataFrame.  Or, by doing inner joins, you can exclude data in the second data frame that doesnt exist in the first.\n",
    "\n",
    "**Comparing data frames:** Sometimes, we need compare two or more dataframes. In this case, join and merge comes in handy.  An inner join between two data frames will get you only the elements in common between two datasets.  Some outer joins will only get you the ones in one or the other dataframe, allowing basically set operations (intersection, etc) to be performed on dataframes.\n",
    "\n",
    "\n",
    "For example, let's say you have one DataFrame that contains information about products for an e-commerce store like product_id, price, category, etc., and a second DataFrame that contains sales data like sale_id, product_id, quantity, customer, etc. If you wanted to find out how many items of each category were sold, you'd need to join these two data frames together on the product_id field.\n",
    "\n",
    "It's important to note that the method you choose depends on the specifics of your situation and what you're trying to achieve. `merge()` is more versatile and can be used in almost any situation, but `join()` is simpler and can be more convenient when you want to combine DataFrames based on their indexes."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fe486a417826e67"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pivot Table\n",
    "\n",
    "Its not as common used, but pivot table and just plain pivot turns out to be somewhat useful in pandas as well.\n",
    "\n",
    "The pivot_table() function is used to create a spreadsheet-style pivot table as a DataFrame in Python pandas. The levels in the pivot table are stored in a MultiIndex object (hierarchical indexes) on the index and columns of the result DataFrame.\n",
    "\n",
    "It may be easiest to understand this if we show an example."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3fb2a574adeec303"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "  City  Temp  Humidity Weather\n0   NY    60        30  Cloudy\n1   NY    58        35   Sunny\n2   LA    75        25   Sunny\n3   LA    76        20   Sunny\n4   NY    59        37    Rain\n5   LA    74        22    Rain",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>City</th>\n      <th>Temp</th>\n      <th>Humidity</th>\n      <th>Weather</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NY</td>\n      <td>60</td>\n      <td>30</td>\n      <td>Cloudy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NY</td>\n      <td>58</td>\n      <td>35</td>\n      <td>Sunny</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LA</td>\n      <td>75</td>\n      <td>25</td>\n      <td>Sunny</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LA</td>\n      <td>76</td>\n      <td>20</td>\n      <td>Sunny</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NY</td>\n      <td>59</td>\n      <td>37</td>\n      <td>Rain</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>LA</td>\n      <td>74</td>\n      <td>22</td>\n      <td>Rain</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Creating a sample DataFrame\n",
    "data = {\n",
    "    \"City\": [\"NY\", \"NY\", \"LA\", \"LA\", \"NY\", \"LA\"],\n",
    "    \"Temp\": [60, 58, 75, 76, 59, 74],\n",
    "    \"Humidity\": [30, 35, 25, 20, 37, 22],\n",
    "    \"Weather\": ['Cloudy', 'Sunny', 'Sunny', 'Sunny', 'Rain', 'Rain']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T00:32:11.572423300Z",
     "start_time": "2023-12-21T00:32:11.544688500Z"
    }
   },
   "id": "a13f8fc946aa8ad4"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "       Humidity  Temp\nCity                 \nLA    22.333333  75.0\nNY    34.000000  59.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Humidity</th>\n      <th>Temp</th>\n    </tr>\n    <tr>\n      <th>City</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>LA</th>\n      <td>22.333333</td>\n      <td>75.0</td>\n    </tr>\n    <tr>\n      <th>NY</th>\n      <td>34.000000</td>\n      <td>59.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot = df.pivot_table(index='City', values=['Temp', 'Humidity'], aggfunc='mean')\n",
    "pivot"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T00:32:50.863572500Z",
     "start_time": "2023-12-21T00:32:50.827050200Z"
    }
   },
   "id": "98b54090e3e07694"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We used the parameter index='City' to group the data by the City column.\n",
    "\n",
    "We specified values=['Temp', 'Humidity'] to calculate the mean temperature and humidity in each city.\n",
    "\n",
    "We used aggfunc='mean' to calculate the mean. If we wanted to calculate the total instead, we could have used 'sum'.\n",
    "\n",
    "The pivot_table function above took a list of city temp and humidity values and twisted the table around until we had an index of unique cities, and in the temp and humidity columns we end up with a mean of the various measurements we calculated.\n",
    "\n",
    "There is a slightly more general purpose version of this function called just `pivot`\n",
    "\n",
    "\n",
    "#### Pivot\n",
    "\n",
    "The pivot() function in pandas is used to reshape data (produce a \"pivot\" table) based on column values. It uses unique values from specified index/ columns to form axes of the resulting DataFrame.\n",
    "\n",
    "Here is the function signature:\n",
    "\n",
    "```python\n",
    "DataFrame.pivot(self, index=None, columns=None, values=None)\n",
    "```\n",
    "\n",
    "Parameters:\n",
    "\n",
    "index: string or object. This is the column in the dataframe that the pivoted data will use as its new index.\n",
    "columns: string or object. The column whose values will become the new columns in the pivoted data.\n",
    "values: string or object. The column to use for populating new frame’s values.\n",
    "\n",
    "An example usage:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7e36ef0cebfe140"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "bar  A  B  C\nfoo         \none  1  2  3\ntwo  4  5  6",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>bar</th>\n      <th>A</th>\n      <th>B</th>\n      <th>C</th>\n    </tr>\n    <tr>\n      <th>foo</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>one</th>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>two</th>\n      <td>4</td>\n      <td>5</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'foo': ['one', 'one', 'one', 'two', 'two', 'two'],\n",
    "    'bar': ['A', 'B', 'C', 'A', 'B', 'C'],\n",
    "    'baz': [1, 2, 3, 4, 5, 6],\n",
    "    'zoo': ['x', 'y', 'z', 'q', 'w', 't']\n",
    "})\n",
    "\n",
    "df.pivot(index='foo', columns='bar', values='baz')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T00:37:26.902381700Z",
     "start_time": "2023-12-21T00:37:26.879155Z"
    }
   },
   "id": "e8342a0a1eb41299"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unlike pivot_table(), the pivot() function does not do any aggregation. It simply reshapes the data as described. If there are any duplicate entries for the index/column combinations, pivot will throw a ValueError.\n",
    "\n",
    "When might one use pivot()?\n",
    "\n",
    "You would use pivot() when you want to move one column's entries to new columns and fill them with corresponding values from another column, but no aggregation operation is needed. It is a simple reshaping of the DataFrame structure.\n",
    "\n",
    "Problems to look out for:\n",
    "\n",
    "Data Size: Pivotting is usually not a good idea if you have a large amount of data. It can significantly increase the amount of data in the DataFrame (since it makes a new column for each unique value), using up a lot of memory.\n",
    "\n",
    "Duplicate Entries: As mentioned earlier pivoting requires the index/column combinations to be unique. If they are not, you will need to use some sort of aggregation (pivot_table() can do this) to combine the duplicate entries in some way.\n",
    "\n",
    "Note: In general, if you have any duplicate values in the columns specified for the 'index' or 'columns' parameters of the pivot() function, it's better to use the pivot_table() function. Otherwise, if your data is clean and well structured without duplicate rows, using the pivot() function is usually faster than applying pivot_table().\n",
    "\n",
    "I often find it useful to use pivot when I want to rotate a series of timeseries values where each row is a time, type, and value, into a multi column dataset where each type gets its own column."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4cc9e581dbf8bbf"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "                   Time    Weather  Values\n0   2022-01-01 00:00:00       temp      51\n1   2022-01-01 01:00:00  windchill      33\n2   2022-01-01 02:00:00       temp      75\n3   2022-01-01 03:00:00  windchill      40\n4   2022-01-01 04:00:00  windchill      96\n..                  ...        ...     ...\n884 2022-02-06 20:00:00   humidity      52\n885 2022-02-06 21:00:00   humidity      18\n886 2022-02-06 22:00:00  windchill      90\n887 2022-02-06 23:00:00   humidity      29\n888 2022-02-07 00:00:00  windchill      15\n\n[889 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>Weather</th>\n      <th>Values</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2022-01-01 00:00:00</td>\n      <td>temp</td>\n      <td>51</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2022-01-01 01:00:00</td>\n      <td>windchill</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2022-01-01 02:00:00</td>\n      <td>temp</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2022-01-01 03:00:00</td>\n      <td>windchill</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2022-01-01 04:00:00</td>\n      <td>windchill</td>\n      <td>96</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>884</th>\n      <td>2022-02-06 20:00:00</td>\n      <td>humidity</td>\n      <td>52</td>\n    </tr>\n    <tr>\n      <th>885</th>\n      <td>2022-02-06 21:00:00</td>\n      <td>humidity</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>886</th>\n      <td>2022-02-06 22:00:00</td>\n      <td>windchill</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>887</th>\n      <td>2022-02-06 23:00:00</td>\n      <td>humidity</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>888</th>\n      <td>2022-02-07 00:00:00</td>\n      <td>windchill</td>\n      <td>15</td>\n    </tr>\n  </tbody>\n</table>\n<p>889 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a date range with hourly frequency\n",
    "date_range = pd.date_range(start='1/1/2022', end='2/7/2022', freq='H')\n",
    "\n",
    "# Create a column with values from the set ('temp', 'humidity', 'windchill')\n",
    "weather = np.random.choice(['temp', 'humidity', 'windchill'], size=len(date_range))\n",
    "\n",
    "# Create a column with random values between 0 to 100\n",
    "values = np.random.randint(0, 100, size=len(date_range))\n",
    "\n",
    "# Construct the dataframe\n",
    "df = pd.DataFrame({\n",
    "    'Time': date_range,\n",
    "    'Weather': weather,\n",
    "    'Values': values\n",
    "})\n",
    "\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T00:41:28.594038600Z",
     "start_time": "2023-12-21T00:41:28.549055200Z"
    }
   },
   "id": "abf38aec5a35616a"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "Weather              humidity  temp  windchill\nTime                                          \n2022-01-01 00:00:00       NaN  51.0        NaN\n2022-01-01 01:00:00       NaN   NaN       33.0\n2022-01-01 02:00:00       NaN  75.0        NaN\n2022-01-01 03:00:00       NaN   NaN       40.0\n2022-01-01 04:00:00       NaN   NaN       96.0\n...                       ...   ...        ...\n2022-02-06 20:00:00      52.0   NaN        NaN\n2022-02-06 21:00:00      18.0   NaN        NaN\n2022-02-06 22:00:00       NaN   NaN       90.0\n2022-02-06 23:00:00      29.0   NaN        NaN\n2022-02-07 00:00:00       NaN   NaN       15.0\n\n[889 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Weather</th>\n      <th>humidity</th>\n      <th>temp</th>\n      <th>windchill</th>\n    </tr>\n    <tr>\n      <th>Time</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2022-01-01 00:00:00</th>\n      <td>NaN</td>\n      <td>51.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2022-01-01 01:00:00</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>33.0</td>\n    </tr>\n    <tr>\n      <th>2022-01-01 02:00:00</th>\n      <td>NaN</td>\n      <td>75.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2022-01-01 03:00:00</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>40.0</td>\n    </tr>\n    <tr>\n      <th>2022-01-01 04:00:00</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>96.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2022-02-06 20:00:00</th>\n      <td>52.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2022-02-06 21:00:00</th>\n      <td>18.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2022-02-06 22:00:00</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>90.0</td>\n    </tr>\n    <tr>\n      <th>2022-02-06 23:00:00</th>\n      <td>29.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2022-02-07 00:00:00</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>15.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>889 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotated = df.pivot(index='Time', columns='Weather', values='Values')\n",
    "rotated"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T00:41:33.732703Z",
     "start_time": "2023-12-21T00:41:33.719892700Z"
    }
   },
   "id": "38e89875529afaca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Resampling time series data.\n",
    "\n",
    "Resampling data sometimes comes up, though not very often.  Note above we created a dataframe with timeseries data in it, but note that the data is rather spares. We have hourly measurements, but never from more than one sensor and only once an hour. What if we wanted to resample this to show a more dense view?\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c21e71af3b64781"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "Weather      humidity       temp  windchill\nTime                                       \n2022-01-01  59.800000  38.900000  57.500000\n2022-01-02  42.000000  39.714286  58.166667\n2022-01-03  35.200000  65.800000  55.714286\n2022-01-04  59.777778  31.000000  46.300000\n2022-01-05  47.545455  47.000000  56.125000\n2022-01-06  44.800000  49.666667  40.857143\n2022-01-07  28.400000  37.833333  48.750000\n2022-01-08  56.125000  46.888889  70.428571\n2022-01-09  49.333333  53.625000  59.900000\n2022-01-10  57.875000  42.875000  42.625000\n2022-01-11  43.125000  46.125000  45.250000\n2022-01-12  40.545455  66.125000  74.200000\n2022-01-13  63.111111  65.250000  48.142857\n2022-01-14  62.714286  65.750000  39.076923\n2022-01-15  49.166667  44.166667  37.833333\n2022-01-16  56.222222  68.200000  50.700000\n2022-01-17  62.888889  56.888889  40.166667\n2022-01-18  42.857143  59.500000  44.666667\n2022-01-19  48.125000  54.000000  40.500000\n2022-01-20  53.454545  50.571429  51.666667\n2022-01-21  54.444444  42.428571  51.375000\n2022-01-22  49.142857  55.777778  45.250000\n2022-01-23  48.666667  47.100000  71.875000\n2022-01-24  30.375000  37.800000  52.909091\n2022-01-25  49.285714  67.500000  36.615385\n2022-01-26  39.875000  37.250000  62.750000\n2022-01-27  50.636364  56.200000  35.125000\n2022-01-28  33.000000  42.666667  60.166667\n2022-01-29  47.444444  28.454545  41.250000\n2022-01-30  50.111111  54.857143  34.125000\n2022-01-31  62.285714  44.142857  64.700000\n2022-02-01  43.375000  58.000000  59.200000\n2022-02-02  51.000000  47.833333  44.714286\n2022-02-03  72.250000  53.285714  33.000000\n2022-02-04  50.500000  46.300000  52.333333\n2022-02-05  40.857143  58.125000  36.111111\n2022-02-06  44.500000  50.800000  70.875000\n2022-02-07        NaN        NaN  15.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Weather</th>\n      <th>humidity</th>\n      <th>temp</th>\n      <th>windchill</th>\n    </tr>\n    <tr>\n      <th>Time</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2022-01-01</th>\n      <td>59.800000</td>\n      <td>38.900000</td>\n      <td>57.500000</td>\n    </tr>\n    <tr>\n      <th>2022-01-02</th>\n      <td>42.000000</td>\n      <td>39.714286</td>\n      <td>58.166667</td>\n    </tr>\n    <tr>\n      <th>2022-01-03</th>\n      <td>35.200000</td>\n      <td>65.800000</td>\n      <td>55.714286</td>\n    </tr>\n    <tr>\n      <th>2022-01-04</th>\n      <td>59.777778</td>\n      <td>31.000000</td>\n      <td>46.300000</td>\n    </tr>\n    <tr>\n      <th>2022-01-05</th>\n      <td>47.545455</td>\n      <td>47.000000</td>\n      <td>56.125000</td>\n    </tr>\n    <tr>\n      <th>2022-01-06</th>\n      <td>44.800000</td>\n      <td>49.666667</td>\n      <td>40.857143</td>\n    </tr>\n    <tr>\n      <th>2022-01-07</th>\n      <td>28.400000</td>\n      <td>37.833333</td>\n      <td>48.750000</td>\n    </tr>\n    <tr>\n      <th>2022-01-08</th>\n      <td>56.125000</td>\n      <td>46.888889</td>\n      <td>70.428571</td>\n    </tr>\n    <tr>\n      <th>2022-01-09</th>\n      <td>49.333333</td>\n      <td>53.625000</td>\n      <td>59.900000</td>\n    </tr>\n    <tr>\n      <th>2022-01-10</th>\n      <td>57.875000</td>\n      <td>42.875000</td>\n      <td>42.625000</td>\n    </tr>\n    <tr>\n      <th>2022-01-11</th>\n      <td>43.125000</td>\n      <td>46.125000</td>\n      <td>45.250000</td>\n    </tr>\n    <tr>\n      <th>2022-01-12</th>\n      <td>40.545455</td>\n      <td>66.125000</td>\n      <td>74.200000</td>\n    </tr>\n    <tr>\n      <th>2022-01-13</th>\n      <td>63.111111</td>\n      <td>65.250000</td>\n      <td>48.142857</td>\n    </tr>\n    <tr>\n      <th>2022-01-14</th>\n      <td>62.714286</td>\n      <td>65.750000</td>\n      <td>39.076923</td>\n    </tr>\n    <tr>\n      <th>2022-01-15</th>\n      <td>49.166667</td>\n      <td>44.166667</td>\n      <td>37.833333</td>\n    </tr>\n    <tr>\n      <th>2022-01-16</th>\n      <td>56.222222</td>\n      <td>68.200000</td>\n      <td>50.700000</td>\n    </tr>\n    <tr>\n      <th>2022-01-17</th>\n      <td>62.888889</td>\n      <td>56.888889</td>\n      <td>40.166667</td>\n    </tr>\n    <tr>\n      <th>2022-01-18</th>\n      <td>42.857143</td>\n      <td>59.500000</td>\n      <td>44.666667</td>\n    </tr>\n    <tr>\n      <th>2022-01-19</th>\n      <td>48.125000</td>\n      <td>54.000000</td>\n      <td>40.500000</td>\n    </tr>\n    <tr>\n      <th>2022-01-20</th>\n      <td>53.454545</td>\n      <td>50.571429</td>\n      <td>51.666667</td>\n    </tr>\n    <tr>\n      <th>2022-01-21</th>\n      <td>54.444444</td>\n      <td>42.428571</td>\n      <td>51.375000</td>\n    </tr>\n    <tr>\n      <th>2022-01-22</th>\n      <td>49.142857</td>\n      <td>55.777778</td>\n      <td>45.250000</td>\n    </tr>\n    <tr>\n      <th>2022-01-23</th>\n      <td>48.666667</td>\n      <td>47.100000</td>\n      <td>71.875000</td>\n    </tr>\n    <tr>\n      <th>2022-01-24</th>\n      <td>30.375000</td>\n      <td>37.800000</td>\n      <td>52.909091</td>\n    </tr>\n    <tr>\n      <th>2022-01-25</th>\n      <td>49.285714</td>\n      <td>67.500000</td>\n      <td>36.615385</td>\n    </tr>\n    <tr>\n      <th>2022-01-26</th>\n      <td>39.875000</td>\n      <td>37.250000</td>\n      <td>62.750000</td>\n    </tr>\n    <tr>\n      <th>2022-01-27</th>\n      <td>50.636364</td>\n      <td>56.200000</td>\n      <td>35.125000</td>\n    </tr>\n    <tr>\n      <th>2022-01-28</th>\n      <td>33.000000</td>\n      <td>42.666667</td>\n      <td>60.166667</td>\n    </tr>\n    <tr>\n      <th>2022-01-29</th>\n      <td>47.444444</td>\n      <td>28.454545</td>\n      <td>41.250000</td>\n    </tr>\n    <tr>\n      <th>2022-01-30</th>\n      <td>50.111111</td>\n      <td>54.857143</td>\n      <td>34.125000</td>\n    </tr>\n    <tr>\n      <th>2022-01-31</th>\n      <td>62.285714</td>\n      <td>44.142857</td>\n      <td>64.700000</td>\n    </tr>\n    <tr>\n      <th>2022-02-01</th>\n      <td>43.375000</td>\n      <td>58.000000</td>\n      <td>59.200000</td>\n    </tr>\n    <tr>\n      <th>2022-02-02</th>\n      <td>51.000000</td>\n      <td>47.833333</td>\n      <td>44.714286</td>\n    </tr>\n    <tr>\n      <th>2022-02-03</th>\n      <td>72.250000</td>\n      <td>53.285714</td>\n      <td>33.000000</td>\n    </tr>\n    <tr>\n      <th>2022-02-04</th>\n      <td>50.500000</td>\n      <td>46.300000</td>\n      <td>52.333333</td>\n    </tr>\n    <tr>\n      <th>2022-02-05</th>\n      <td>40.857143</td>\n      <td>58.125000</td>\n      <td>36.111111</td>\n    </tr>\n    <tr>\n      <th>2022-02-06</th>\n      <td>44.500000</td>\n      <td>50.800000</td>\n      <td>70.875000</td>\n    </tr>\n    <tr>\n      <th>2022-02-07</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>15.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotated.resample('D').mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T00:44:50.773388600Z",
     "start_time": "2023-12-21T00:44:50.715948Z"
    }
   },
   "id": "b7009eba1ef79c95"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Above, we asked pandas to resample this dataset so we got daily average of the time series data, rather than that sparse set of data.  We didnt need hourly data, not really, not in this example, but getting a daily average was good enough.\n",
    "\n",
    "It is possible to get more complicated though, if thats what you want."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "895f1a2ad902f1d8"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "Weather    humidity                              temp                   \\\n                min   max       mean        std   min   max       mean   \nTime                                                                     \n2022-01-01      0.0  87.0  59.800000  27.635726   8.0  75.0  38.900000   \n2022-01-02      7.0  90.0  42.000000  31.249000   7.0  71.0  39.714286   \n2022-01-03      8.0  68.0  35.200000  26.224035  42.0  86.0  65.800000   \n2022-01-04     20.0  99.0  59.777778  24.524364   4.0  88.0  31.000000   \n2022-01-05      8.0  87.0  47.545455  27.890370  10.0  89.0  47.000000   \n2022-01-06     24.0  81.0  44.800000  21.718656   1.0  98.0  49.666667   \n2022-01-07      3.0  55.0  28.400000  20.369912   8.0  64.0  37.833333   \n2022-01-08     16.0  99.0  56.125000  30.390024   0.0  98.0  46.888889   \n2022-01-09      3.0  89.0  49.333333  30.715903  16.0  80.0  53.625000   \n2022-01-10      7.0  99.0  57.875000  34.774529  10.0  98.0  42.875000   \n2022-01-11      1.0  99.0  43.125000  34.852495  17.0  89.0  46.125000   \n2022-01-12      8.0  65.0  40.545455  22.818254   6.0  96.0  66.125000   \n2022-01-13     28.0  99.0  63.111111  27.369894  39.0  92.0  65.250000   \n2022-01-14     17.0  99.0  62.714286  28.581546  18.0  94.0  65.750000   \n2022-01-15      8.0  77.0  49.166667  23.659389   3.0  96.0  44.166667   \n2022-01-16      1.0  99.0  56.222222  34.175202  28.0  90.0  68.200000   \n2022-01-17     22.0  94.0  62.888889  24.967201  11.0  81.0  56.888889   \n2022-01-18     11.0  72.0  42.857143  23.926475  17.0  91.0  59.500000   \n2022-01-19      0.0  97.0  48.125000  32.843514  16.0  83.0  54.000000   \n2022-01-20      8.0  98.0  53.454545  28.079757  12.0  87.0  50.571429   \n2022-01-21     15.0  95.0  54.444444  29.875203  13.0  81.0  42.428571   \n2022-01-22      2.0  97.0  49.142857  35.063887  18.0  98.0  55.777778   \n2022-01-23      3.0  96.0  48.666667  39.878148   1.0  80.0  47.100000   \n2022-01-24      0.0  84.0  30.375000  27.197361   3.0  70.0  37.800000   \n2022-01-25     12.0  86.0  49.285714  26.942620  56.0  77.0  67.500000   \n2022-01-26     10.0  87.0  39.875000  29.107866   8.0  71.0  37.250000   \n2022-01-27      3.0  98.0  50.636364  36.216772  36.0  98.0  56.200000   \n2022-01-28      0.0  95.0  33.000000  30.232433   1.0  96.0  42.666667   \n2022-01-29      7.0  98.0  47.444444  28.373011   2.0  68.0  28.454545   \n2022-01-30      9.0  94.0  50.111111  32.887857   6.0  97.0  54.857143   \n2022-01-31     18.0  91.0  62.285714  27.010580  14.0  77.0  44.142857   \n2022-02-01      4.0  99.0  43.375000  34.846346  18.0  82.0  58.000000   \n2022-02-02     15.0  98.0  51.000000  28.993103  17.0  79.0  47.833333   \n2022-02-03     39.0  98.0  72.250000  27.741365   0.0  95.0  53.285714   \n2022-02-04     11.0  94.0  50.500000  30.687364   8.0  99.0  46.300000   \n2022-02-05      7.0  96.0  40.857143  33.879338  17.0  94.0  58.125000   \n2022-02-06     18.0  80.0  44.500000  25.406692  13.0  75.0  50.800000   \n2022-02-07      NaN   NaN        NaN        NaN   NaN   NaN        NaN   \n\nWeather               windchill                              \n                  std       min   max       mean        std  \nTime                                                         \n2022-01-01  23.923257      33.0  96.0  57.500000  28.290163  \n2022-01-02  22.178926      16.0  88.0  58.166667  24.248086  \n2022-01-03  15.786070       8.0  97.0  55.714286  27.594393  \n2022-01-04  33.518652       7.0  96.0  46.300000  29.386505  \n2022-01-05  36.448594      18.0  94.0  56.125000  33.060280  \n2022-01-06  33.232879       4.0  92.0  40.857143  33.358800  \n2022-01-07  21.132124       9.0  95.0  48.750000  30.363277  \n2022-01-08  38.650499      41.0  87.0  70.428571  18.310809  \n2022-01-09  18.844761       3.0  99.0  59.900000  31.423452  \n2022-01-10  31.498016       2.0  77.0  42.625000  29.683750  \n2022-01-11  25.255480       4.0  98.0  45.250000  37.178143  \n2022-01-12  31.674404      64.0  92.0  74.200000  11.189281  \n2022-01-13  20.561754       3.0  85.0  48.142857  29.373620  \n2022-01-14  35.349446       1.0  77.0  39.076923  22.863587  \n2022-01-15  28.161332       0.0  66.0  37.833333  21.414170  \n2022-01-16  27.626075      10.0  91.0  50.700000  30.206879  \n2022-01-17  21.945640      10.0  96.0  40.166667  34.862109  \n2022-01-18  29.553825      12.0  91.0  44.666667  28.407745  \n2022-01-19  19.994444       3.0  70.0  40.500000  24.484689  \n2022-01-20  29.859991       9.0  88.0  51.666667  32.103998  \n2022-01-21  25.389349       8.0  84.0  51.375000  26.381473  \n2022-01-22  28.734899       0.0  84.0  45.250000  32.176966  \n2022-01-23  26.614324      46.0  93.0  71.875000  17.365298  \n2022-01-24  32.606748       8.0  96.0  52.909091  31.033706  \n2022-01-25   8.888194       3.0  76.0  36.615385  23.935812  \n2022-01-26  22.776868      33.0  99.0  62.750000  21.298893  \n2022-01-27  25.994230      17.0  72.0  35.125000  18.976960  \n2022-01-28  36.667424      34.0  79.0  60.166667  20.595307  \n2022-01-29  24.695601       3.0  90.0  41.250000  39.229028  \n2022-01-30  34.095105       3.0  89.0  34.125000  26.405830  \n2022-01-31  27.027323      24.0  86.0  64.700000  19.994722  \n2022-02-01  22.600885       1.0  98.0  59.200000  35.773361  \n2022-02-02  22.247846      22.0  71.0  44.714286  19.128637  \n2022-02-03  40.119464       2.0  91.0  33.000000  28.290163  \n2022-02-04  31.710671       8.0  98.0  52.333333  37.940304  \n2022-02-05  28.707577       5.0  79.0  36.111111  24.064728  \n2022-02-06  20.481970      30.0  95.0  70.875000  24.044230  \n2022-02-07        NaN      15.0  15.0  15.000000        NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th>Weather</th>\n      <th colspan=\"4\" halign=\"left\">humidity</th>\n      <th colspan=\"4\" halign=\"left\">temp</th>\n      <th colspan=\"4\" halign=\"left\">windchill</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>min</th>\n      <th>max</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>max</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>max</th>\n      <th>mean</th>\n      <th>std</th>\n    </tr>\n    <tr>\n      <th>Time</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2022-01-01</th>\n      <td>0.0</td>\n      <td>87.0</td>\n      <td>59.800000</td>\n      <td>27.635726</td>\n      <td>8.0</td>\n      <td>75.0</td>\n      <td>38.900000</td>\n      <td>23.923257</td>\n      <td>33.0</td>\n      <td>96.0</td>\n      <td>57.500000</td>\n      <td>28.290163</td>\n    </tr>\n    <tr>\n      <th>2022-01-02</th>\n      <td>7.0</td>\n      <td>90.0</td>\n      <td>42.000000</td>\n      <td>31.249000</td>\n      <td>7.0</td>\n      <td>71.0</td>\n      <td>39.714286</td>\n      <td>22.178926</td>\n      <td>16.0</td>\n      <td>88.0</td>\n      <td>58.166667</td>\n      <td>24.248086</td>\n    </tr>\n    <tr>\n      <th>2022-01-03</th>\n      <td>8.0</td>\n      <td>68.0</td>\n      <td>35.200000</td>\n      <td>26.224035</td>\n      <td>42.0</td>\n      <td>86.0</td>\n      <td>65.800000</td>\n      <td>15.786070</td>\n      <td>8.0</td>\n      <td>97.0</td>\n      <td>55.714286</td>\n      <td>27.594393</td>\n    </tr>\n    <tr>\n      <th>2022-01-04</th>\n      <td>20.0</td>\n      <td>99.0</td>\n      <td>59.777778</td>\n      <td>24.524364</td>\n      <td>4.0</td>\n      <td>88.0</td>\n      <td>31.000000</td>\n      <td>33.518652</td>\n      <td>7.0</td>\n      <td>96.0</td>\n      <td>46.300000</td>\n      <td>29.386505</td>\n    </tr>\n    <tr>\n      <th>2022-01-05</th>\n      <td>8.0</td>\n      <td>87.0</td>\n      <td>47.545455</td>\n      <td>27.890370</td>\n      <td>10.0</td>\n      <td>89.0</td>\n      <td>47.000000</td>\n      <td>36.448594</td>\n      <td>18.0</td>\n      <td>94.0</td>\n      <td>56.125000</td>\n      <td>33.060280</td>\n    </tr>\n    <tr>\n      <th>2022-01-06</th>\n      <td>24.0</td>\n      <td>81.0</td>\n      <td>44.800000</td>\n      <td>21.718656</td>\n      <td>1.0</td>\n      <td>98.0</td>\n      <td>49.666667</td>\n      <td>33.232879</td>\n      <td>4.0</td>\n      <td>92.0</td>\n      <td>40.857143</td>\n      <td>33.358800</td>\n    </tr>\n    <tr>\n      <th>2022-01-07</th>\n      <td>3.0</td>\n      <td>55.0</td>\n      <td>28.400000</td>\n      <td>20.369912</td>\n      <td>8.0</td>\n      <td>64.0</td>\n      <td>37.833333</td>\n      <td>21.132124</td>\n      <td>9.0</td>\n      <td>95.0</td>\n      <td>48.750000</td>\n      <td>30.363277</td>\n    </tr>\n    <tr>\n      <th>2022-01-08</th>\n      <td>16.0</td>\n      <td>99.0</td>\n      <td>56.125000</td>\n      <td>30.390024</td>\n      <td>0.0</td>\n      <td>98.0</td>\n      <td>46.888889</td>\n      <td>38.650499</td>\n      <td>41.0</td>\n      <td>87.0</td>\n      <td>70.428571</td>\n      <td>18.310809</td>\n    </tr>\n    <tr>\n      <th>2022-01-09</th>\n      <td>3.0</td>\n      <td>89.0</td>\n      <td>49.333333</td>\n      <td>30.715903</td>\n      <td>16.0</td>\n      <td>80.0</td>\n      <td>53.625000</td>\n      <td>18.844761</td>\n      <td>3.0</td>\n      <td>99.0</td>\n      <td>59.900000</td>\n      <td>31.423452</td>\n    </tr>\n    <tr>\n      <th>2022-01-10</th>\n      <td>7.0</td>\n      <td>99.0</td>\n      <td>57.875000</td>\n      <td>34.774529</td>\n      <td>10.0</td>\n      <td>98.0</td>\n      <td>42.875000</td>\n      <td>31.498016</td>\n      <td>2.0</td>\n      <td>77.0</td>\n      <td>42.625000</td>\n      <td>29.683750</td>\n    </tr>\n    <tr>\n      <th>2022-01-11</th>\n      <td>1.0</td>\n      <td>99.0</td>\n      <td>43.125000</td>\n      <td>34.852495</td>\n      <td>17.0</td>\n      <td>89.0</td>\n      <td>46.125000</td>\n      <td>25.255480</td>\n      <td>4.0</td>\n      <td>98.0</td>\n      <td>45.250000</td>\n      <td>37.178143</td>\n    </tr>\n    <tr>\n      <th>2022-01-12</th>\n      <td>8.0</td>\n      <td>65.0</td>\n      <td>40.545455</td>\n      <td>22.818254</td>\n      <td>6.0</td>\n      <td>96.0</td>\n      <td>66.125000</td>\n      <td>31.674404</td>\n      <td>64.0</td>\n      <td>92.0</td>\n      <td>74.200000</td>\n      <td>11.189281</td>\n    </tr>\n    <tr>\n      <th>2022-01-13</th>\n      <td>28.0</td>\n      <td>99.0</td>\n      <td>63.111111</td>\n      <td>27.369894</td>\n      <td>39.0</td>\n      <td>92.0</td>\n      <td>65.250000</td>\n      <td>20.561754</td>\n      <td>3.0</td>\n      <td>85.0</td>\n      <td>48.142857</td>\n      <td>29.373620</td>\n    </tr>\n    <tr>\n      <th>2022-01-14</th>\n      <td>17.0</td>\n      <td>99.0</td>\n      <td>62.714286</td>\n      <td>28.581546</td>\n      <td>18.0</td>\n      <td>94.0</td>\n      <td>65.750000</td>\n      <td>35.349446</td>\n      <td>1.0</td>\n      <td>77.0</td>\n      <td>39.076923</td>\n      <td>22.863587</td>\n    </tr>\n    <tr>\n      <th>2022-01-15</th>\n      <td>8.0</td>\n      <td>77.0</td>\n      <td>49.166667</td>\n      <td>23.659389</td>\n      <td>3.0</td>\n      <td>96.0</td>\n      <td>44.166667</td>\n      <td>28.161332</td>\n      <td>0.0</td>\n      <td>66.0</td>\n      <td>37.833333</td>\n      <td>21.414170</td>\n    </tr>\n    <tr>\n      <th>2022-01-16</th>\n      <td>1.0</td>\n      <td>99.0</td>\n      <td>56.222222</td>\n      <td>34.175202</td>\n      <td>28.0</td>\n      <td>90.0</td>\n      <td>68.200000</td>\n      <td>27.626075</td>\n      <td>10.0</td>\n      <td>91.0</td>\n      <td>50.700000</td>\n      <td>30.206879</td>\n    </tr>\n    <tr>\n      <th>2022-01-17</th>\n      <td>22.0</td>\n      <td>94.0</td>\n      <td>62.888889</td>\n      <td>24.967201</td>\n      <td>11.0</td>\n      <td>81.0</td>\n      <td>56.888889</td>\n      <td>21.945640</td>\n      <td>10.0</td>\n      <td>96.0</td>\n      <td>40.166667</td>\n      <td>34.862109</td>\n    </tr>\n    <tr>\n      <th>2022-01-18</th>\n      <td>11.0</td>\n      <td>72.0</td>\n      <td>42.857143</td>\n      <td>23.926475</td>\n      <td>17.0</td>\n      <td>91.0</td>\n      <td>59.500000</td>\n      <td>29.553825</td>\n      <td>12.0</td>\n      <td>91.0</td>\n      <td>44.666667</td>\n      <td>28.407745</td>\n    </tr>\n    <tr>\n      <th>2022-01-19</th>\n      <td>0.0</td>\n      <td>97.0</td>\n      <td>48.125000</td>\n      <td>32.843514</td>\n      <td>16.0</td>\n      <td>83.0</td>\n      <td>54.000000</td>\n      <td>19.994444</td>\n      <td>3.0</td>\n      <td>70.0</td>\n      <td>40.500000</td>\n      <td>24.484689</td>\n    </tr>\n    <tr>\n      <th>2022-01-20</th>\n      <td>8.0</td>\n      <td>98.0</td>\n      <td>53.454545</td>\n      <td>28.079757</td>\n      <td>12.0</td>\n      <td>87.0</td>\n      <td>50.571429</td>\n      <td>29.859991</td>\n      <td>9.0</td>\n      <td>88.0</td>\n      <td>51.666667</td>\n      <td>32.103998</td>\n    </tr>\n    <tr>\n      <th>2022-01-21</th>\n      <td>15.0</td>\n      <td>95.0</td>\n      <td>54.444444</td>\n      <td>29.875203</td>\n      <td>13.0</td>\n      <td>81.0</td>\n      <td>42.428571</td>\n      <td>25.389349</td>\n      <td>8.0</td>\n      <td>84.0</td>\n      <td>51.375000</td>\n      <td>26.381473</td>\n    </tr>\n    <tr>\n      <th>2022-01-22</th>\n      <td>2.0</td>\n      <td>97.0</td>\n      <td>49.142857</td>\n      <td>35.063887</td>\n      <td>18.0</td>\n      <td>98.0</td>\n      <td>55.777778</td>\n      <td>28.734899</td>\n      <td>0.0</td>\n      <td>84.0</td>\n      <td>45.250000</td>\n      <td>32.176966</td>\n    </tr>\n    <tr>\n      <th>2022-01-23</th>\n      <td>3.0</td>\n      <td>96.0</td>\n      <td>48.666667</td>\n      <td>39.878148</td>\n      <td>1.0</td>\n      <td>80.0</td>\n      <td>47.100000</td>\n      <td>26.614324</td>\n      <td>46.0</td>\n      <td>93.0</td>\n      <td>71.875000</td>\n      <td>17.365298</td>\n    </tr>\n    <tr>\n      <th>2022-01-24</th>\n      <td>0.0</td>\n      <td>84.0</td>\n      <td>30.375000</td>\n      <td>27.197361</td>\n      <td>3.0</td>\n      <td>70.0</td>\n      <td>37.800000</td>\n      <td>32.606748</td>\n      <td>8.0</td>\n      <td>96.0</td>\n      <td>52.909091</td>\n      <td>31.033706</td>\n    </tr>\n    <tr>\n      <th>2022-01-25</th>\n      <td>12.0</td>\n      <td>86.0</td>\n      <td>49.285714</td>\n      <td>26.942620</td>\n      <td>56.0</td>\n      <td>77.0</td>\n      <td>67.500000</td>\n      <td>8.888194</td>\n      <td>3.0</td>\n      <td>76.0</td>\n      <td>36.615385</td>\n      <td>23.935812</td>\n    </tr>\n    <tr>\n      <th>2022-01-26</th>\n      <td>10.0</td>\n      <td>87.0</td>\n      <td>39.875000</td>\n      <td>29.107866</td>\n      <td>8.0</td>\n      <td>71.0</td>\n      <td>37.250000</td>\n      <td>22.776868</td>\n      <td>33.0</td>\n      <td>99.0</td>\n      <td>62.750000</td>\n      <td>21.298893</td>\n    </tr>\n    <tr>\n      <th>2022-01-27</th>\n      <td>3.0</td>\n      <td>98.0</td>\n      <td>50.636364</td>\n      <td>36.216772</td>\n      <td>36.0</td>\n      <td>98.0</td>\n      <td>56.200000</td>\n      <td>25.994230</td>\n      <td>17.0</td>\n      <td>72.0</td>\n      <td>35.125000</td>\n      <td>18.976960</td>\n    </tr>\n    <tr>\n      <th>2022-01-28</th>\n      <td>0.0</td>\n      <td>95.0</td>\n      <td>33.000000</td>\n      <td>30.232433</td>\n      <td>1.0</td>\n      <td>96.0</td>\n      <td>42.666667</td>\n      <td>36.667424</td>\n      <td>34.0</td>\n      <td>79.0</td>\n      <td>60.166667</td>\n      <td>20.595307</td>\n    </tr>\n    <tr>\n      <th>2022-01-29</th>\n      <td>7.0</td>\n      <td>98.0</td>\n      <td>47.444444</td>\n      <td>28.373011</td>\n      <td>2.0</td>\n      <td>68.0</td>\n      <td>28.454545</td>\n      <td>24.695601</td>\n      <td>3.0</td>\n      <td>90.0</td>\n      <td>41.250000</td>\n      <td>39.229028</td>\n    </tr>\n    <tr>\n      <th>2022-01-30</th>\n      <td>9.0</td>\n      <td>94.0</td>\n      <td>50.111111</td>\n      <td>32.887857</td>\n      <td>6.0</td>\n      <td>97.0</td>\n      <td>54.857143</td>\n      <td>34.095105</td>\n      <td>3.0</td>\n      <td>89.0</td>\n      <td>34.125000</td>\n      <td>26.405830</td>\n    </tr>\n    <tr>\n      <th>2022-01-31</th>\n      <td>18.0</td>\n      <td>91.0</td>\n      <td>62.285714</td>\n      <td>27.010580</td>\n      <td>14.0</td>\n      <td>77.0</td>\n      <td>44.142857</td>\n      <td>27.027323</td>\n      <td>24.0</td>\n      <td>86.0</td>\n      <td>64.700000</td>\n      <td>19.994722</td>\n    </tr>\n    <tr>\n      <th>2022-02-01</th>\n      <td>4.0</td>\n      <td>99.0</td>\n      <td>43.375000</td>\n      <td>34.846346</td>\n      <td>18.0</td>\n      <td>82.0</td>\n      <td>58.000000</td>\n      <td>22.600885</td>\n      <td>1.0</td>\n      <td>98.0</td>\n      <td>59.200000</td>\n      <td>35.773361</td>\n    </tr>\n    <tr>\n      <th>2022-02-02</th>\n      <td>15.0</td>\n      <td>98.0</td>\n      <td>51.000000</td>\n      <td>28.993103</td>\n      <td>17.0</td>\n      <td>79.0</td>\n      <td>47.833333</td>\n      <td>22.247846</td>\n      <td>22.0</td>\n      <td>71.0</td>\n      <td>44.714286</td>\n      <td>19.128637</td>\n    </tr>\n    <tr>\n      <th>2022-02-03</th>\n      <td>39.0</td>\n      <td>98.0</td>\n      <td>72.250000</td>\n      <td>27.741365</td>\n      <td>0.0</td>\n      <td>95.0</td>\n      <td>53.285714</td>\n      <td>40.119464</td>\n      <td>2.0</td>\n      <td>91.0</td>\n      <td>33.000000</td>\n      <td>28.290163</td>\n    </tr>\n    <tr>\n      <th>2022-02-04</th>\n      <td>11.0</td>\n      <td>94.0</td>\n      <td>50.500000</td>\n      <td>30.687364</td>\n      <td>8.0</td>\n      <td>99.0</td>\n      <td>46.300000</td>\n      <td>31.710671</td>\n      <td>8.0</td>\n      <td>98.0</td>\n      <td>52.333333</td>\n      <td>37.940304</td>\n    </tr>\n    <tr>\n      <th>2022-02-05</th>\n      <td>7.0</td>\n      <td>96.0</td>\n      <td>40.857143</td>\n      <td>33.879338</td>\n      <td>17.0</td>\n      <td>94.0</td>\n      <td>58.125000</td>\n      <td>28.707577</td>\n      <td>5.0</td>\n      <td>79.0</td>\n      <td>36.111111</td>\n      <td>24.064728</td>\n    </tr>\n    <tr>\n      <th>2022-02-06</th>\n      <td>18.0</td>\n      <td>80.0</td>\n      <td>44.500000</td>\n      <td>25.406692</td>\n      <td>13.0</td>\n      <td>75.0</td>\n      <td>50.800000</td>\n      <td>20.481970</td>\n      <td>30.0</td>\n      <td>95.0</td>\n      <td>70.875000</td>\n      <td>24.044230</td>\n    </tr>\n    <tr>\n      <th>2022-02-07</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>15.0</td>\n      <td>15.0</td>\n      <td>15.000000</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotated.resample('D').agg(['min', 'max', 'mean', 'std'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T00:44:32.102294700Z",
     "start_time": "2023-12-21T00:44:31.994231300Z"
    }
   },
   "id": "d3821eb2564ce721"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2593587f63661320"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
